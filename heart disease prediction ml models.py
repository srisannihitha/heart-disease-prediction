# -*- coding: utf-8 -*-
"""DIC PROJECT2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d4yzDawHVTNiBgFHCnppo-qFgDiYN611
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold
from sklearn.preprocessing import StandardScaler
# %config InlineBackend.figure_format = 'svg'
from imblearn.over_sampling import SMOTE
import itertools

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.linear_model import LogisticRegression

#from google.colab import drive
#drive.mount('/content/drive')

df= pd.read_csv(r'/content/cleaned_data.csv')

y=df['HeartDisease'].values
X=df.drop('HeartDisease', axis=1).values

X_balance,y_balance = SMOTE().fit_resample(X,y)
print(X_balance.shape)
print(y_balance.shape)

X_train, X_test, y_train, y_test = train_test_split(X_balance,y_balance, test_size=0.3)

"""**LOGISTIC REGRESSION**"""

lr = LogisticRegression()
lr.fit(X_train, y_train)
y_predict_lr = lr.predict(X_test)
lr_acc =accuracy_score(y_test, y_predict_lr)
print('Accuracy Score is {:.2}'.format(accuracy_score(y_test, y_predict_lr)))
print('F1 Score is {:.2}'.format(f1_score(y_test, y_predict_lr)))
print('Precission Score is {:.2}'.format(precision_score(y_test, y_predict_lr)))
print('Recall Score is {:.2}'.format(recall_score(y_test, y_predict_lr)))

cm = confusion_matrix(y_test, y_predict_lr)
cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
fig, ax = plt.subplots(figsize=(6,5))
sns.heatmap(cm_norm, annot=True, cmap="Blues")
ax.set(title="Normalized Confusion Matrix for LogisticRegression",
       xlabel="Predicted",
       ylabel="Actual")
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")

fpr, tpr, thresholds = roc_curve(y_test, y_predict_lr)
auc = roc_auc_score(y_test, y_predict_lr)


plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, label=f'Logistic Regression (AUC={auc:.2f})', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')

plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve - Logistic Regression')

plt.grid(True)
plt.legend()

optimal_threshold_index = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_threshold_index]
plt.annotate(f'Optimal Threshold = {optimal_threshold:.2f}',
             xy=(fpr[optimal_threshold_index], tpr[optimal_threshold_index]),
             xytext=(fpr[optimal_threshold_index] + 0.1, tpr[optimal_threshold_index] - 0.1),
             arrowprops=dict(facecolor='black', arrowstyle='->'))

plt.show()

coefficients = np.abs(lr.coef_[0])
sorted_indices = np.argsort(coefficients)[::-1]
feature_names = ['Smoking', 'AlcoholDrinking', 'Stroke', 'PhysicalHealth', 'MentalHealth', 'DiffWalking',
                 'Sex', 'Age', 'Race', 'Diabetic', 'PhysicalActivity', 'GenHealth', 'Asthma',
                 'KidneyDisease', 'SkinCancer', 'BodyMassIndex_scaled']
colors = ['blue', 'green', 'red', 'purple', 'orange', 'yellow', 'cyan', 'magenta', 'brown', 'pink',
          'lightblue', 'lightgreen', 'lightcoral', 'lightskyblue', 'lightyellow', 'lightpink']

plt.figure(figsize=(6, 6))
bars = plt.bar(range(len(coefficients)), coefficients[sorted_indices], color=[colors[i] for i in sorted_indices])
plt.xticks(range(len(coefficients)), np.array(feature_names)[sorted_indices], rotation=45, ha='right')
plt.xlabel('Feature')
plt.ylabel('Absolute Coefficient')
plt.title('Logistic Regression Feature Importances')
plt.tight_layout()

legend_labels = np.array(feature_names)[sorted_indices]
plt.legend(bars, legend_labels, loc='upper right')
plt.show()

"""**DECISION TREE**"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split

model_dtc = DecisionTreeClassifier(max_depth=5, min_samples_split=5, min_samples_leaf=2)

model_dtc.fit(X_train, y_train)

y_predict_dtc = model_dtc.predict(X_test)

dtc_acc = accuracy_score(y_test, y_predict_dtc)
print('Accuracy Score is {:.5}'.format(dtc_acc))
print('F1 Score is {:.3}'.format(f1_score(y_test, y_predict_dtc)))
print('Precision Score is {:.3}'.format(precision_score(y_test, y_predict_dtc)))
print('Recall Score is {:.3}'.format(recall_score(y_test, y_predict_dtc)))

cm = confusion_matrix(y_test, y_predict_dtc)
cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
fig, ax = plt.subplots(figsize=(4,4))
sns.heatmap(cm_norm, annot=True, cmap="Blues")
ax.set(title="Normalized Confusion Matrix for DecisionTreeClassifier",
       xlabel="Predicted",
       ylabel="Actual")
plt.show()

fpr, tpr, thresholds = roc_curve(y_test, y_predict_dtc)
auc = roc_auc_score(y_test, y_predict_dtc)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, label=f'DecisionTreeClassifier (AUC={auc:.2f})', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve : DecisionTreeClassifier')
plt.grid(True)
plt.legend()

optimal_threshold_index = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_threshold_index]
plt.annotate(f'Optimal Threshold = {optimal_threshold:.2f}',
             xy=(fpr[optimal_threshold_index], tpr[optimal_threshold_index]),
             xytext=(fpr[optimal_threshold_index] + 0.1, tpr[optimal_threshold_index] - 0.1),
             arrowprops=dict(facecolor='black', arrowstyle='->'))

plt.show()

"""**RANDOM FOREST**"""

from sklearn.ensemble import RandomForestClassifier
model_rfc = RandomForestClassifier(n_estimators=500, max_depth=None, random_state=42, n_jobs=-1)
model_rfc.fit(X_train, y_train)
y_predict_rfc = model_rfc.predict(X_test)
rfc_acc=accuracy_score(y_test, y_predict_rfc)
print('Accuracy Score is {:.5}'.format(accuracy_score(y_test, y_predict_rfc)))
print('F1 Score is {:.3}'.format(f1_score(y_test, y_predict_rfc)))
print('Precission Score is {:.3}'.format(precision_score(y_test, y_predict_rfc)))
print('Recall Score is {:.3}'.format(recall_score(y_test, y_predict_rfc)))

cm = confusion_matrix(y_test, y_predict_rfc)
cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
fig, ax = plt.subplots(figsize=(4,4))
sns.heatmap(cm_norm, annot=True, cmap="Blues")
ax.set(title="RandomForestClassifier",
       xlabel="Predicted",
       ylabel="Actual")
plt.show()

fpr, tpr, thresholds = roc_curve(y_test, y_predict_rfc)
auc = roc_auc_score(y_test, y_predict_rfc)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, label=f'RandomForestClassifier (AUC={auc:.2f})', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve : RandomForestClassifier')
plt.grid(True)
plt.legend()

optimal_threshold_index = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_threshold_index]
plt.annotate(f'Optimal Threshold = {optimal_threshold:.2f}',
             xy=(fpr[optimal_threshold_index], tpr[optimal_threshold_index]),
             xytext=(fpr[optimal_threshold_index] + 0.1, tpr[optimal_threshold_index] - 0.1),
             arrowprops=dict(facecolor='black', arrowstyle='->'))

plt.show()

from sklearn.metrics import precision_recall_curve
precision, recall, thresholds = precision_recall_curve(y_test, y_predict_rfc)
plt.plot(recall, precision)
plt.title('Precision-Recall Curve : Random Forest Classifier')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.show()

importance_scores = model_rfc.feature_importances_
feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importance_scores})
sns.set_style("whitegrid")
fig, ax = plt.subplots(figsize=(6, 6))
sns.barplot(x="Importance", y="Feature", data=feature_importance, ax=ax, orient='h')
plt.title('Feature Importance : Random Forest Classifier')
plt.show()

"""**KNN CLASSIFIER**"""

from sklearn.neighbors import KNeighborsClassifier
model_knn = KNeighborsClassifier()
model_knn.fit(X_train, y_train)
y_predict_knn = model_knn.predict(X_test)
knn_acc= accuracy_score(y_test, y_predict_knn)
print('Accuracy Score is {:.3}'.format(accuracy_score(y_test, y_predict_knn)))
print('F1 Score is {:.3}'.format(f1_score(y_test, y_predict_knn)))
print('Precission Score is {:.3}'.format(precision_score(y_test, y_predict_knn)))
print('Recall Score is {:.3}'.format(recall_score(y_test, y_predict_knn)))

cm = confusion_matrix(y_test, y_predict_knn)
cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
fig, ax = plt.subplots(figsize=(5,5))
sns.heatmap(cm_norm, annot=True, cmap="Blues")
ax.set(title="Normalized Confusion Matrix for KNeighbors Classifier",
       xlabel="Predicted",
       ylabel="Actual")
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_test, y_predict_knn)
auc = roc_auc_score(y_test, y_predict_knn)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, label=f'KNeighbors Classifier (AUC={auc:.2f})', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve : KNeighbors Classifier')
plt.grid(True)
plt.legend()

optimal_threshold_index = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_threshold_index]
plt.annotate(f'Optimal Threshold = {optimal_threshold:.2f}',
             xy=(fpr[optimal_threshold_index], tpr[optimal_threshold_index]),
             xytext=(fpr[optimal_threshold_index] + 0.1, tpr[optimal_threshold_index] - 0.1),
             arrowprops=dict(facecolor='black', arrowstyle='->'))

plt.show()

"""**XGB CLASSIFIER**"""

from xgboost import XGBClassifier
model_xgb = XGBClassifier(n_estimators= 300 , max_depth= 3 , learning_rate = 0.1)
model_xgb.fit(X_train, y_train)
y_predict_xgb = model_xgb.predict(X_test)
xgb_acc=accuracy_score(y_test, y_predict_xgb)
print('Accuracy Score is {:.5}'.format(accuracy_score(y_test, y_predict_xgb)))
print('F1 Score is {:.3}'.format(f1_score(y_test, y_predict_xgb)))
print('Precission Score is {:.3}'.format(precision_score(y_test, y_predict_xgb)))
print('Recall Score is {:.3}'.format(recall_score(y_test, y_predict_xgb)))

cm = confusion_matrix(y_test, y_predict_xgb)
cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
fig, ax = plt.subplots(figsize=(5,5))
sns.heatmap(cm_norm, annot=True, cmap="Blues")
ax.set(title="Normalized Confusion Matrix for XGBClassifier",
       xlabel="Predicted",
       ylabel="Actual")
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_test, y_predict_xgb)
auc = roc_auc_score(y_test, y_predict_xgb)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, label=f'XGBClassifier (AUC={auc:.2f})', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve : XGBClassifier')
plt.grid(True)
plt.legend()

optimal_threshold_index = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_threshold_index]
plt.annotate(f'Optimal Threshold = {optimal_threshold:.2f}',
             xy=(fpr[optimal_threshold_index], tpr[optimal_threshold_index]),
             xytext=(fpr[optimal_threshold_index] + 0.1, tpr[optimal_threshold_index] - 0.1),
             arrowprops=dict(facecolor='black', arrowstyle='->'))

plt.show()

importance_scores = model_xgb.feature_importances_
feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importance_scores})
sns.set_style("whitegrid")
fig, ax = plt.subplots(figsize=(6, 6))
sns.barplot(x="Importance", y="Feature", data=feature_importance, ax=ax, orient='h')
plt.title('Feature Importance : XGBClassifier')
plt.show()

"""**ADA BOOST**"""

from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification

model_adaboost = AdaBoostClassifier()

model_adaboost.fit(X_train, y_train)

y_predict_adaboost = model_adaboost.predict(X_test)

adaboost_acc = accuracy_score(y_test, y_predict_adaboost)
print('AdaBoost - Accuracy Score is {:.5}'.format(adaboost_acc))
print('AdaBoost - F1 Score is {:.3}'.format(f1_score(y_test, y_predict_adaboost)))
print('AdaBoost - Precision Score is {:.3}'.format(precision_score(y_test, y_predict_adaboost)))
print('AdaBoost - Recall Score is {:.3}'.format(recall_score(y_test, y_predict_adaboost)))

cm = confusion_matrix(y_test, y_predict_adaboost)
cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
fig, ax = plt.subplots(figsize=(5,5))
sns.heatmap(cm_norm, annot=True, cmap="Blues")
ax.set(title="Normalized Confusion Matrix for AdaBoost Classifier",
       xlabel="Predicted",
       ylabel="Actual")
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt


fpr, tpr, thresholds = roc_curve(y_test, y_predict_adaboost )
auc = roc_auc_score(y_test, y_predict_adaboost)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, label=f'XGBClassifier (AUC={auc:.2f})', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve : ADA BOOST Classifier')
plt.grid(True)
plt.legend()

optimal_threshold_index = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_threshold_index]
plt.annotate(f'Optimal Threshold = {optimal_threshold:.2f}',
             xy=(fpr[optimal_threshold_index], tpr[optimal_threshold_index]),
             xytext=(fpr[optimal_threshold_index] + 0.1, tpr[optimal_threshold_index] - 0.1),
             arrowprops=dict(facecolor='black', arrowstyle='->'))

plt.show()

"""**CAT BOOST**"""

!pip install catboost
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification


model_catboost = CatBoostClassifier()

model_catboost.fit(X_train, y_train)

y_predict_catboost = model_catboost.predict(X_test)

catboost_acc = accuracy_score(y_test, y_predict_catboost)
print('CatBoost - Accuracy Score is {:.5}'.format(catboost_acc))
print('CatBoost - F1 Score is {:.3}'.format(f1_score(y_test, y_predict_catboost)))
print('CatBoost - Precision Score is {:.3}'.format(precision_score(y_test, y_predict_catboost)))
print('CatBoost - Recall Score is {:.3}'.format(recall_score(y_test, y_predict_catboost)))

cm = confusion_matrix(y_test, y_predict_catboost)
cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
fig, ax = plt.subplots(figsize=(5,5))
sns.heatmap(cm_norm, annot=True, cmap="Blues")
ax.set(title="Normalized Confusion Matrix for CAT Classifier",
       xlabel="Predicted",
       ylabel="Actual")
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_test, y_predict_catboost )
auc = roc_auc_score(y_test, y_predict_catboost)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, label=f'XGBClassifier (AUC={auc:.2f})', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve : ADA BOOST Classifier')
plt.grid(True)
plt.legend()

optimal_threshold_index = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_threshold_index]
plt.annotate(f'Optimal Threshold = {optimal_threshold:.2f}',
             xy=(fpr[optimal_threshold_index], tpr[optimal_threshold_index]),
             xytext=(fpr[optimal_threshold_index] + 0.1, tpr[optimal_threshold_index] - 0.1),
             arrowprops=dict(facecolor='black', arrowstyle='->'))

plt.show()

model = [ 'Logistic Reg', 'DT','RF','KNN','AdaBoost', 'XG', 'CAT']
accuracy= [lr_acc, dtc_acc, rfc_acc, knn_acc ,adaboost_acc,xgb_acc,catboost_acc  ]
colors = ['lightblue', 'orange', 'lightgreen', 'green', 'lightpink', 'green']
plt.figure(figsize=(8,6))
plt.bar(model, accuracy,color=colors)
plt.title('Accuracy Scores of 7 Heart Disease Predictor Models')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.show()